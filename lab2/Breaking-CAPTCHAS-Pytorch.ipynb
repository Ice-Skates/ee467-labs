{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09e684a",
   "metadata": {},
   "source": [
    "# Breaking CAPTCHAs — PyTorch\n",
    "\n",
    "This notebook provides a PyTorch implementation of the character-classifier CNN (LeNet-style) that matches the common TensorFlow/Keras architecture used in the lab.\n",
    "\n",
    "**Model:**\n",
    "- Input: (1, 20, 20)\n",
    "- Conv(20, 5×5, padding='same') → ReLU → MaxPool(2)\n",
    "- Conv(50, 5×5, padding='same') → ReLU → MaxPool(2)\n",
    "- Flatten → FC(500) → ReLU → FC(n_classes)\n",
    "\n",
    "> **Important:** Set `CHAR_IMAGE_FOLDER` to the folder that contains your per-class subfolders (e.g., `A/`, `B/`, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have PyTorch installed in this environment, run:\n",
    "# !pip install torch torchvision\n",
    "\n",
    "import os, pickle, math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils import paths\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fb6ba",
   "metadata": {},
   "source": [
    "## Helpers (preprocessing)\n",
    "\n",
    "These mirror typical lab helper functions: resize to 20×20, scale to [0,1], and use **channel-first** `(1,20,20)` for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_fit(image, width, height):\n",
    "    (h, w) = image.shape[:2]\n",
    "    if w > h:\n",
    "        image = imutils.resize(image, width=width)\n",
    "    else:\n",
    "        image = imutils.resize(image, height=height)\n",
    "\n",
    "    padW = int((width - image.shape[1]) / 2.0)\n",
    "    padH = int((height - image.shape[0]) / 2.0)\n",
    "\n",
    "    image = cv2.copyMakeBorder(image, padH, padH, padW, padW, cv2.BORDER_REPLICATE)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    return image\n",
    "\n",
    "def make_feature(image_gray_2d):\n",
    "    \"\"\"Return a PyTorch-ready feature: float32 in [0,1] with shape (1,20,20).\"\"\"\n",
    "    image_resized = resize_to_fit(image_gray_2d, 20, 20)\n",
    "    x = image_resized.astype(np.float32) / 255.0\n",
    "    x = np.expand_dims(x, axis=0)  # (1,20,20)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a23f6c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Assumes character images are stored in a directory structure like:\n",
    "\n",
    "```\n",
    "CHAR_IMAGE_FOLDER/\n",
    "  A/xxx.png\n",
    "  B/yyy.png\n",
    "  ...\n",
    "```\n",
    "\n",
    "The label is taken from the **parent directory name** of each image path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc63272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, char_image_folder, label_encoder):\n",
    "        self.image_paths = list(paths.list_images(char_image_folder))\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        self.labels_str = [p.split(os.path.sep)[-2] for p in self.image_paths]\n",
    "        self.labels = self.label_encoder.transform(self.labels_str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Could not read image: {p}\")\n",
    "        x = make_feature(img)  # (1,20,20)\n",
    "        y = self.labels[idx]   # integer class index\n",
    "        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4c7fc",
   "metadata": {},
   "source": [
    "## Configure paths + build train/validation loaders\n",
    "\n",
    "Set `CHAR_IMAGE_FOLDER` to your character dataset folder.\n",
    "\n",
    "If you get `num_images = 0`, your path is wrong or the dataset isn't extracted where you think it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set this to match your lab's dataset location\n",
    "CHAR_IMAGE_FOLDER = \"./characters\"\n",
    "LABELS_PATH = \"./labels.pkl\"\n",
    "\n",
    "# Sanity check\n",
    "all_paths = list(paths.list_images(CHAR_IMAGE_FOLDER))\n",
    "print(\"CHAR_IMAGE_FOLDER:\", CHAR_IMAGE_FOLDER)\n",
    "print(\"num_images:\", len(all_paths))\n",
    "print(\"sample_paths:\", all_paths[:3])\n",
    "\n",
    "# Build label encoder based on folder names\n",
    "labels_str_all = [p.split(os.path.sep)[-2] for p in all_paths]\n",
    "le = LabelEncoder()\n",
    "le.fit(labels_str_all)\n",
    "n_classes = len(le.classes_)\n",
    "print(\"n_classes:\", n_classes)\n",
    "\n",
    "# Save the label mapping (optional, like the TF lab)\n",
    "with open(LABELS_PATH, \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "dataset = CharDataset(CHAR_IMAGE_FOLDER, le)\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.25, random_state=955996, shuffle=True)\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb08e0",
   "metadata": {},
   "source": [
    "## Model (PyTorch)\n",
    "\n",
    "This matches the common Keras LeNet-style model used in many CAPTCHA labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)   # same padding for 5x5\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, padding=2)  # same padding\n",
    "\n",
    "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
    "        self.fc2 = nn.Linear(500, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B,20,10,10)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B,50,5,5)\n",
    "        x = torch.flatten(x, 1)               # (B,1250)\n",
    "        x = F.relu(self.fc1(x))               # (B,500)\n",
    "        x = self.fc2(x)                       # (B,n_classes) logits\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edfc74",
   "metadata": {},
   "source": [
    "## Train + validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ac8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "model = CaptchaCNN(n_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def accuracy(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    return correct / total if total else 0.0\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "\n",
    "    train_acc = accuracy(train_loader)\n",
    "    val_acc = accuracy(val_loader)\n",
    "    avg_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{N_EPOCHS} | loss={avg_loss:.4f} | train_acc={train_acc:.4f} | val_acc={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709258db",
   "metadata": {},
   "source": [
    "## Save model weights (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8625c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./captcha-model-pytorch.pt\")\n",
    "print(\"Saved:\", \"./captcha-model-pytorch.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88151a48",
   "metadata": {},
   "source": [
    "## Optional: Predict character labels with the trained model\n",
    "\n",
    "If you already have a list of extracted character images (each a 2D grayscale 20×20 image), you can batch-predict them and convert indices back to label strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chars_torch(char_images_20x20_gray):\n",
    "    feats = np.stack([make_feature(img) for img in char_images_20x20_gray], axis=0)  # (N,1,20,20)\n",
    "    x = torch.from_numpy(feats).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        pred_idx = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    return le.inverse_transform(pred_idx)\n",
    "\n",
    "# Example usage:\n",
    "# pred_labels = predict_chars_torch(list_of_20x20_gray_images)\n",
    "# print(pred_labels[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
